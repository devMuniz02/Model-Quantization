{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a24f419c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üöÄ QUANTIZATION LAB: bert-base-uncased\n",
      "============================================================\n",
      "üì¶ Original Footprint (FLOAT32): 438.07 MB\n",
      "üõ†Ô∏è  Loading in 4bit mode...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 202/202 [00:00<00:00, 739.86it/s, Materializing param=cls.predictions.transform.dense.weight]                 \n",
      "\u001b[1mBertForMaskedLM LOAD REPORT\u001b[0m from: bert-base-uncased\n",
      "Key                         | Status     |  | \n",
      "----------------------------+------------+--+-\n",
      "bert.pooler.dense.weight    | UNEXPECTED |  | \n",
      "cls.seq_relationship.bias   | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight | UNEXPECTED |  | \n",
      "bert.pooler.dense.bias      | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä COMPARISON:\n",
      "üì¶ Original (FLOAT32):  438.07 MB\n",
      "‚úÖ Quantized:           138.73 MB\n",
      "üìâ Actual Reduction:    68.3%\n",
      "üè∑Ô∏è  Suggested HF Name:  bert-base-uncased-bnb-4bit-nf4-dq\n",
      "\n",
      "üíæ Saving to ./bert-base-uncased-bnb-4bit-nf4-dq...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing model shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  9.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully saved as bert-base-uncased-bnb-4bit-nf4-dq with metadata injection!\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoModelForMaskedLM, AutoConfig, BitsAndBytesConfig\n",
    "\n",
    "def run_quantization_lab(model_id, mode=\"4bit\", double_quant=True, quant_type=\"nf4\"):\n",
    "    \"\"\"\n",
    "    Precision-aware workbench that detects native dtype (FP32/FP16/BF16),\n",
    "    quantizes the model, and saves with standardized HF initials.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\\nüöÄ QUANTIZATION LAB: {model_id}\\n{'='*60}\")\n",
    "\n",
    "    # --- 1. DETECT NATIVE DTYPE & CALC ORIGINAL FOOTPRINT ---\n",
    "    config = AutoConfig.from_pretrained(model_id, trust_remote_code=True)\n",
    "    \n",
    "    # Check what the model's actual native dtype is\n",
    "    native_dtype = getattr(config, \"torch_dtype\", torch.float32)\n",
    "    if native_dtype is None: native_dtype = torch.float32 # Default if not specified\n",
    "    \n",
    "    # Virtual load on 'meta' device using the native dtype\n",
    "    with torch.device(\"meta\"):\n",
    "        is_masked = any(arch in str(config.architectures) for arch in [\"Masked\", \"Bert\", \"Roberta\"])\n",
    "        model_class = AutoModelForMaskedLM if is_masked else AutoModelForCausalLM\n",
    "        temp_model = model_class.from_config(config, torch_dtype=native_dtype)\n",
    "    \n",
    "    orig_mem = temp_model.get_memory_footprint() / 1e6\n",
    "    dtype_name = str(native_dtype).split('.')[-1].upper()\n",
    "    print(f\"üì¶ Original Footprint ({dtype_name}): {orig_mem:.2f} MB\")\n",
    "\n",
    "    # --- 2. CONFIGURATION & NAMING ---\n",
    "    dq_init = \"-dq\" if (mode == \"4bit\" and double_quant) else \"\"\n",
    "    type_init = f\"-{quant_type}\" if mode == \"4bit\" else \"\"\n",
    "    # Generates name like: bert-base-uncased-bnb-4bit-nf4-dq\n",
    "    hf_save_name = f\"{model_id.split('/')[-1]}-bnb-{mode}{type_init}{dq_init}\"\n",
    "    \n",
    "    if mode == \"4bit\":\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=quant_type,\n",
    "            bnb_4bit_use_double_quant=double_quant,\n",
    "            bnb_4bit_compute_dtype=torch.float16 # Standard for 4bit compute\n",
    "        )\n",
    "    elif mode == \"8bit\":\n",
    "        bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "    else:\n",
    "        bnb_config = None\n",
    "\n",
    "    # --- 3. LOAD QUANTIZED MODEL ---\n",
    "    print(f\"üõ†Ô∏è  Loading in {mode} mode...\")\n",
    "    model = model_class.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    # --- 4. FINAL PRECISION-AWARE REPORT ---\n",
    "    quant_mem = model.get_memory_footprint() / 1e6\n",
    "    reduction = ((orig_mem - quant_mem) / orig_mem) * 100\n",
    "\n",
    "    print(f\"\\nüìä COMPARISON:\")\n",
    "    print(f\"üì¶ Original ({dtype_name}):  {orig_mem:.2f} MB\")\n",
    "    print(f\"‚úÖ Quantized:           {quant_mem:.2f} MB\")\n",
    "    print(f\"üìâ Actual Reduction:    {reduction:.1f}%\")\n",
    "    print(f\"üè∑Ô∏è  Suggested HF Name:  {hf_save_name}\")\n",
    "\n",
    "    # --- 5. SAVE WITH METADATA ---\n",
    "    save_path = f\"./{hf_save_name}\"\n",
    "    print(f\"\\nüíæ Saving to {save_path}...\")\n",
    "    model.save_pretrained(save_path)\n",
    "    \n",
    "    # Inject bnb_config into config.json for auto-loading\n",
    "    config_file = os.path.join(save_path, \"config.json\")\n",
    "    with open(config_file, \"r\") as f:\n",
    "        saved_json = json.load(f)\n",
    "    \n",
    "    saved_json[\"quantization_config\"] = bnb_config.to_dict() if bnb_config else {}\n",
    "    \n",
    "    with open(config_file, \"w\") as f:\n",
    "        json.dump(saved_json, f, indent=2)\n",
    "    \n",
    "    print(f\"‚úÖ Successfully saved as {hf_save_name} with metadata injection!\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    return model\n",
    "\n",
    "# --- EXECUTION ---\n",
    "model = run_quantization_lab(\n",
    "    model_id=\"bert-base-uncased\", \n",
    "    mode=\"4bit\", \n",
    "    double_quant=True, \n",
    "    quant_type=\"nf4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccc2b49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
