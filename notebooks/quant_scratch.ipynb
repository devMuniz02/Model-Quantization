{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c3de859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Initializing GPT-2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 148/148 [00:00<00:00, 1548.70it/s, Materializing param=transformer.wte.weight]             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Original Model Footprint: 474.70 MB\n",
      "\n",
      "--- Quantizing with GPTQ ---\n",
      "--- Quantizing with AWQ ---\n",
      "\n",
      "==================================================\n",
      "MODEL FOOTPRINTS:\n",
      "Original (FP32):          474.70 MB\n",
      "GPTQ Quantized (INT4):     59.34 MB\n",
      "AWQ Quantized (INT4):      59.34 MB\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import copy\n",
    "import os\n",
    "\n",
    "# --- UTILS: Memory Footprint Calculation ---\n",
    "def get_model_size_mb(model, bits=None):\n",
    "    \"\"\"Calculates the approximate memory footprint of the model parameters.\"\"\"\n",
    "    if bits is None:\n",
    "        total_bits = 0\n",
    "        for name, param in model.named_parameters():\n",
    "            # Handle cases where weights might be stored as different dtypes\n",
    "            bits_per_param = torch.finfo(param.dtype).bits if param.is_floating_point() else torch.iinfo(param.dtype).bits\n",
    "            total_bits += param.numel() * bits_per_param\n",
    "    else:\n",
    "        total_bits = sum(param.numel() for name, param in model.named_parameters()) * bits\n",
    "    return total_bits / (8 * 1024 * 1024)\n",
    "\n",
    "# --- CORE MATH: GPTQ (Hessian-based Correction) ---\n",
    "def gptq_quantize_weights(W, X, bits=4):\n",
    "    \"\"\"\n",
    "    W: [Out_features, In_features] (Original Weight)\n",
    "    X: [Batch, Seq, In_features] (Calibration Activations)\n",
    "    \"\"\"\n",
    "    # 1. Flatten and prepare X\n",
    "    X = X.reshape(-1, X.shape[-1]).t().float() \n",
    "    in_features = W.shape[1]\n",
    "    \n",
    "    # 2. Compute Inverse Hessian (H = XX^T)\n",
    "    H = torch.matmul(X, X.t())\n",
    "    damp = 0.01 * torch.mean(torch.diag(H))\n",
    "    H_inv = torch.inverse(H + damp * torch.eye(in_features))\n",
    "    \n",
    "    W_quant = W.clone().float()\n",
    "    \n",
    "    # 3. Step through columns and compensate error\n",
    "    for i in range(in_features):\n",
    "        w_col = W_quant[:, i]\n",
    "        \n",
    "        # Symmetric Quantization (Signed)\n",
    "        scale = w_col.abs().max() / (2**(bits-1) - 1)\n",
    "        w_q = torch.round(w_col / scale).clamp(-2**(bits-1), 2**(bits-1) - 1) * scale\n",
    "        \n",
    "        # Update neighbors using Hessian to 'absorb' quantization error\n",
    "        error = w_col - w_q\n",
    "        update_factor = H_inv[i, i+1:] / H_inv[i, i]\n",
    "        W_quant[:, i+1:] -= error.unsqueeze(1) * update_factor.unsqueeze(0)\n",
    "        W_quant[:, i] = w_q\n",
    "        \n",
    "    return W_quant\n",
    "\n",
    "# --- CORE MATH: AWQ (Activation-aware Scaling) ---\n",
    "def awq_quantize_weights(W, act_means, bits=4):\n",
    "    \"\"\"\n",
    "    W: [Out_features, In_features]\n",
    "    act_means: [In_features] (Average magnitude of activations)\n",
    "    \"\"\"\n",
    "    # Salience heuristic: prioritize weights connected to high-magnitude inputs\n",
    "    # Scaling protects these 'salient' weights from rounding error\n",
    "    scales = act_means.pow(0.5) / (W.abs().mean(dim=0).pow(0.5) + 1e-8)\n",
    "    scales = scales / scales.max() \n",
    "    \n",
    "    # Shield -> Quantize -> Unshield\n",
    "    W_scaled = W * scales.view(1, -1)\n",
    "    q_scale = W_scaled.abs().max() / (2**(bits-1) - 1)\n",
    "    W_q = torch.round(W_scaled / q_scale).clamp(-2**(bits-1), 2**(bits-1) - 1) * q_scale\n",
    "    \n",
    "    return W_q / scales.view(1, -1)\n",
    "\n",
    "# --- Quantize Whole Model ---\n",
    "def quantize_model_gptq(model, tokenizer, bits=4):\n",
    "    model_q = copy.deepcopy(model)\n",
    "    activations = {}\n",
    "    hooks = []\n",
    "    \n",
    "    def hook_fn(name):\n",
    "        def hook(m, i, o): activations[name] = i[0].detach()\n",
    "        return hook\n",
    "    \n",
    "    # Register hooks for all Conv1D layers\n",
    "    for name, module in model_q.named_modules():\n",
    "        if 'Conv1D' in str(type(module)):\n",
    "            hooks.append(module.register_forward_hook(hook_fn(name)))\n",
    "    \n",
    "    # Run calibration\n",
    "    inputs = tokenizer(\"Quantization is the process of reducing precision to make models smaller and faster.\", return_tensors=\"pt\")\n",
    "    model_q(inputs.input_ids)\n",
    "    for h in hooks: h.remove()\n",
    "    \n",
    "    # Quantize each layer\n",
    "    for name, module in model_q.named_modules():\n",
    "        if 'Conv1D' in str(type(module)):\n",
    "            X = activations[name]\n",
    "            W_orig = module.weight.data.t()  # [out, in]\n",
    "            W_q = gptq_quantize_weights(W_orig, X, bits)\n",
    "            module.weight.data = W_q.t()  # back to [in, out]\n",
    "    \n",
    "    return model_q\n",
    "\n",
    "def quantize_model_awq(model, tokenizer, bits=4):\n",
    "    model_q = copy.deepcopy(model)\n",
    "    activations = {}\n",
    "    hooks = []\n",
    "    \n",
    "    def hook_fn(name):\n",
    "        def hook(m, i, o): activations[name] = i[0].detach()\n",
    "        return hook\n",
    "    \n",
    "    # Register hooks for all Conv1D layers\n",
    "    for name, module in model_q.named_modules():\n",
    "        if 'Conv1D' in str(type(module)):\n",
    "            hooks.append(module.register_forward_hook(hook_fn(name)))\n",
    "    \n",
    "    # Run calibration\n",
    "    inputs = tokenizer(\"Quantization is the process of reducing precision to make models smaller and faster.\", return_tensors=\"pt\")\n",
    "    model_q(inputs.input_ids)\n",
    "    for h in hooks: h.remove()\n",
    "    \n",
    "    # Quantize each layer\n",
    "    for name, module in model_q.named_modules():\n",
    "        if 'Conv1D' in str(type(module)):\n",
    "            X = activations[name]\n",
    "            act_means = X.abs().mean(dim=(0, 1))\n",
    "            W_orig = module.weight.data.t()  # [out, in]\n",
    "            W_q = awq_quantize_weights(W_orig, act_means, bits)\n",
    "            module.weight.data = W_q.t()  # back to [in, out]\n",
    "    \n",
    "    return model_q\n",
    "\n",
    "# --- EXECUTION PIPELINE ---\n",
    "def main():\n",
    "    print(\"ðŸš€ Initializing GPT-2...\")\n",
    "    model_id = \"gpt2\"\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_id)\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_id)\n",
    "    \n",
    "    orig_size = get_model_size_mb(model)\n",
    "    print(f\"ðŸ“¦ Original Model Footprint: {orig_size:.2f} MB\")\n",
    "    \n",
    "    # Quantize with GPTQ\n",
    "    print(\"\\n--- Quantizing with GPTQ ---\")\n",
    "    model_gptq = quantize_model_gptq(model, tokenizer, bits=4)\n",
    "    gptq_size = get_model_size_mb(model_gptq, bits=4)\n",
    "    \n",
    "    # Quantize with AWQ\n",
    "    print(\"--- Quantizing with AWQ ---\")\n",
    "    model_awq = quantize_model_awq(model, tokenizer, bits=4)\n",
    "    awq_size = get_model_size_mb(model_awq, bits=4)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"MODEL FOOTPRINTS:\")\n",
    "    print(f\"Original (FP32):         {orig_size:7.2f} MB\")\n",
    "    print(f\"GPTQ Quantized (INT4):   {gptq_size:7.2f} MB\")\n",
    "    print(f\"AWQ Quantized (INT4):    {awq_size:7.2f} MB\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05dac678",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 148/148 [00:00<00:00, 1550.87it/s, Materializing param=transformer.wte.weight]             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method       | Bits  | Size (MB)  | Perplexity\n",
      "--------------------------------------------------\n",
      "Baseline     | 32    | 500.0      | 84.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 148/148 [00:00<00:00, 1583.63it/s, Materializing param=transformer.wte.weight]             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTQ         | 16    | 237.4      | 84.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 148/148 [00:00<00:00, 1591.15it/s, Materializing param=transformer.wte.weight]             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWQ          | 16    | 237.4      | 84.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 148/148 [00:00<00:00, 1452.04it/s, Materializing param=transformer.wte.weight]             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTQ         | 8     | 118.7      | 86.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 148/148 [00:00<00:00, 1307.33it/s, Materializing param=transformer.wte.weight]             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWQ          | 8     | 118.7      | 83.43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 148/148 [00:00<00:00, 1310.68it/s, Materializing param=transformer.wte.weight]             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTQ         | 4     | 59.3       | 652.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 148/148 [00:00<00:00, 1312.23it/s, Materializing param=transformer.wte.weight]             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWQ          | 4     | 59.3       | 102.95\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import math\n",
    "import copy\n",
    "\n",
    "# --- CORE MATH UTILS ---\n",
    "\n",
    "def quantize_group(x, bits, group_size=128):\n",
    "    if bits >= 16: return x.half().float()\n",
    "    orig_shape = x.shape\n",
    "    x = x.reshape(-1, group_size)\n",
    "    q_max = 2**(bits - 1) - 1\n",
    "    scales = x.abs().max(dim=1, keepdim=True)[0] / q_max\n",
    "    x_q = torch.round(x / (scales + 1e-8)).clamp(-q_max, q_max) * scales\n",
    "    return x_q.reshape(orig_shape)\n",
    "\n",
    "def apply_gptq(W, X, bits, group_size=128):\n",
    "    \"\"\"Hessian-based Error Compensation\"\"\"\n",
    "    if bits >= 16: return W.half().float()\n",
    "    X = X.reshape(-1, X.shape[-1]).t().float()\n",
    "    in_features = W.shape[1]\n",
    "    H = torch.matmul(X, X.t())\n",
    "    damp = 0.1 * torch.mean(torch.diag(H))\n",
    "    H_inv = torch.inverse(H + damp * torch.eye(in_features))\n",
    "    W_q = W.clone().float()\n",
    "    for i in range(in_features):\n",
    "        w_col = W_q[:, i]\n",
    "        w_rounded = quantize_group(w_col.unsqueeze(0), bits, len(w_col)).squeeze(0)\n",
    "        error = w_col - w_rounded\n",
    "        W_q[:, i+1:] -= error.unsqueeze(1) * (H_inv[i, i+1:] / H_inv[i, i]).unsqueeze(0)\n",
    "        W_q[:, i] = w_rounded\n",
    "    return W_q\n",
    "\n",
    "def apply_awq(W, X, bits, group_size=128):\n",
    "    \"\"\"Activation-aware Scaling\"\"\"\n",
    "    if bits >= 16: return W.half().float()\n",
    "    act_means = X.abs().mean(dim=(0, 1))\n",
    "    # Alpha search for best scaling\n",
    "    best_error, best_W = float('inf'), W\n",
    "    for alpha in [0.5]: # Standard AWQ alpha\n",
    "        scales = act_means.pow(alpha) / (W.abs().mean(dim=0).pow(1-alpha) + 1e-8)\n",
    "        scales = scales / scales.max()\n",
    "        W_scaled = W * scales.view(1, -1)\n",
    "        W_q = quantize_group(W_scaled, bits, group_size) / scales.view(1, -1)\n",
    "        error = torch.nn.functional.mse_loss(W_q @ X[0].t(), W @ X[0].t())\n",
    "        if error < best_error:\n",
    "            best_error, best_W = error, W_q\n",
    "    return best_W\n",
    "\n",
    "# --- MODEL PROCESSING ENGINE ---\n",
    "\n",
    "def get_ppl(model, tokenizer, text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        loss = model(inputs.input_ids, labels=inputs.input_ids).loss\n",
    "    return math.exp(loss.item())\n",
    "\n",
    "def run_experiment(model_id, method, bits):\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_id)\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_id)\n",
    "    test_text = \"The artificial intelligence revolution is driven by efficient algorithms.\"\n",
    "    \n",
    "    # Calibration\n",
    "    activations = {}\n",
    "    def hook_fn(name):\n",
    "        def hook(m, i, o): activations[name] = i[0].detach()\n",
    "        return hook\n",
    "    \n",
    "    hooks = [m.register_forward_hook(hook_fn(n)) for n, m in model.named_modules() if \"Conv1D\" in str(type(m))]\n",
    "    model(tokenizer(\"Quantization improves inference speed.\", return_tensors=\"pt\").input_ids)\n",
    "    for h in hooks: h.remove()\n",
    "\n",
    "    # Quantize\n",
    "    with torch.no_grad():\n",
    "        for name, m in model.named_modules():\n",
    "            if \"Conv1D\" in str(type(m)) and name in activations:\n",
    "                W = m.weight.data.t()\n",
    "                X = activations[name]\n",
    "                W_q = apply_gptq(W, X, bits) if method == \"GPTQ\" else apply_awq(W, X, bits)\n",
    "                m.weight.copy_(W_q.t())\n",
    "                \n",
    "    ppl = get_ppl(model, tokenizer, test_text)\n",
    "    size = (sum(p.numel() for p in model.parameters()) * bits) / (8 * 1024 * 1024)\n",
    "    return ppl, size\n",
    "\n",
    "# --- MAIN RUN ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    m_id = \"gpt2\"\n",
    "    tok = GPT2Tokenizer.from_pretrained(m_id)\n",
    "    base_model = GPT2LMHeadModel.from_pretrained(m_id)\n",
    "    test_p = \"The artificial intelligence revolution is driven by efficient algorithms.\"\n",
    "    \n",
    "    print(f\"{'Method':<12} | {'Bits':<5} | {'Size (MB)':<10} | {'Perplexity':<10}\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'Baseline':<12} | {'32':<5} | {500.0:<10.1f} | {get_ppl(base_model, tok, test_p):.2f}\")\n",
    "\n",
    "    for b in [16, 8, 4]:\n",
    "        for meth in [\"GPTQ\", \"AWQ\"]:\n",
    "            p, s = run_experiment(m_id, meth, b)\n",
    "            print(f\"{meth:<12} | {b:<5} | {s:<10.1f} | {p:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada07ef6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
