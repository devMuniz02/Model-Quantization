{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03007a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limpiando caché...\n",
      "Espacio total que se liberará: 18.5G\n",
      "¡Caché de modelos completamente limpia!\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import scan_cache_dir\n",
    "\n",
    "# 1. Scan the entire current cache\n",
    "cache_info = scan_cache_dir()\n",
    "\n",
    "# 2. Create a deletion strategy that includes ALL revisions of ALL repositories\n",
    "delete_strategy = cache_info.delete_revisions(*[\n",
    "    revision.commit_hash \n",
    "    for repo in cache_info.repos \n",
    "    for revision in repo.revisions\n",
    "])\n",
    "\n",
    "# 3. Show how much space will be freed before executing\n",
    "print(f\"Cleaning cache...\")\n",
    "print(f\"Total space to be freed: {delete_strategy.expected_freed_size_str}\")\n",
    "\n",
    "# 4. Execute the cleanup\n",
    "delete_strategy.execute()\n",
    "\n",
    "print(\"Model cache completely cleaned!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc7e43b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\emman\\Desktop\\PROYECTOS_VS_CODE\\GITHUB\\Model-Quantization\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TABLE 1: TOP 50 TRENDING (ALL TYPES)\n",
      "Model ID                                                | Type         | Params\n",
      "--------------------------------------------------------------------------------\n",
      "zai-org/GLM-5                                           | Base         | 753.9B\n",
      "openbmb/MiniCPM-o-4_5                                   | Base         | 9.4B\n",
      "openbmb/MiniCPM-SALA                                    | Base         | 9.5B\n",
      "Qwen/Qwen3-Coder-Next                                   | Base         | 79.7B\n",
      "moonshotai/Kimi-K2.5                                    | Base         | 170.7B\n",
      "zai-org/GLM-OCR                                         | Base         | N/A\n",
      "mistralai/Voxtral-Mini-4B-Realtime-2602                 | Fine-tuned   | N/A\n",
      "Nanbeige/Nanbeige4.1-3B                                 | Fine-tuned   | 3.9B\n",
      "ACE-Step/Ace-Step1.5                                    | Base         | N/A\n",
      "circlestone-labs/Anima                                  | Base         | N/A\n",
      "stepfun-ai/Step-3.5-Flash                               | Fine-tuned   | 199.4B\n",
      "unsloth/Qwen3-Coder-Next-GGUF                           | GGUF         | N/A\n",
      "inclusionAI/Ming-flash-omni-2.0                         | Base         | 104.2B\n",
      "inference-net/Schematron-3B                             | Fine-tuned   | 3.2B\n",
      "TeichAI/GLM-4.7-Flash-Claude-Opus-4.5-High-Reasoning-Distill-GGUF | GGUF         | N/A\n",
      "UCSB-SURFI/VulnLLM-R-7B                                 | Fine-tuned   | 7.6B\n",
      "internlm/Intern-S1-Pro                                  | Base         | N/A\n",
      "ytu-ce-cosmos/Turkish-Gemma-9b-T1                       | Fine-tuned   | 9.2B\n",
      "DavidAU/GLM-4.7-Flash-Uncensored-Heretic-NEO-CODE-Imatrix-MAX-GGUF | GGUF         | N/A\n",
      "tencent/HunyuanImage-3.0-Instruct                       | Base         | 83.0B\n",
      "unsloth/GLM-5-GGUF                                      | GGUF         | N/A\n",
      "nvidia/personaplex-7b-v1                                | Fine-tuned   | N/A\n",
      "Qwen/Qwen3-ASR-1.7B                                     | Base         | 2.3B\n",
      "zai-org/GLM-4.7-Flash                                   | Base         | 31.2B\n",
      "kugelaudio/kugelaudio-0-open                            | Base         | 9.3B\n",
      "Lightricks/LTX-2                                        | Base         | N/A\n",
      "FutureMa/Eva-4B-V2                                      | Fine-tuned   | 4.0B\n",
      "Octen/Octen-Embedding-8B                                | Fine-tuned   | 7.6B\n",
      "Qwen/Qwen3-Coder-Next-GGUF                              | GGUF         | N/A\n",
      "meta-llama/Llama-3.1-8B-Instruct                        | Fine-tuned   | 8.0B\n",
      "Tongyi-MAI/Z-Image                                      | Base         | N/A\n",
      "Tongyi-MAI/Z-Image-Turbo                                | Base         | N/A\n",
      "zai-org/GLM-5-FP8                                       | Base         | 753.9B\n",
      "Alissonerdx/BFS-Best-Face-Swap-Video                    | Fine-tuned   | N/A\n",
      "openbmb/MiniCPM-o-4_5-gguf                              | GGUF         | N/A\n",
      "Qwen/Qwen3-TTS-12Hz-1.7B-CustomVoice                    | Base         | 1.9B\n",
      "deepseek-ai/DeepSeek-OCR-2                              | Base         | 3.4B\n",
      "inclusionAI/LLaDA2.1-mini                               | Base         | 16.3B\n",
      "Soul-AILab/SoulX-Singer                                 | Base         | N/A\n",
      "openai/gpt-oss-20b                                      | Base         | 21.5B\n",
      "AIDC-AI/Ovis2.6-30B-A3B                                 | Base         | 31.4B\n",
      "fal/Qwen-Image-Edit-2511-Multiple-Angles-LoRA           | LoRA         | N/A\n",
      "PaddlePaddle/PaddleOCR-VL-1.5                           | Fine-tuned   | 958.6M\n",
      "Phr00t/LTX2-Rapid-Merges                                | Fine-tuned   | N/A\n",
      "microsoft/VibeVoice-ASR                                 | Base         | 8.7B\n",
      "inclusionAI/LLaDA2.1-flash                              | Base         | 102.9B\n",
      "trillionlabs/gWorld-8B                                  | Base         | 8.8B\n",
      "alibaba-pai/Z-Image-Fun-Lora-Distill                    | LoRA         | N/A\n",
      "unsloth/GLM-4.7-Flash-GGUF                              | GGUF         | N/A\n",
      "zai-org/GLM-4.7                                         | Base         | 358.3B\n",
      "\n",
      "================================================================================\n",
      "\n",
      "TABLE 2: BASE & FINE-TUNED MODELS (< 10B PARAMS)\n",
      "Model ID                                                | Type         | Params\n",
      "--------------------------------------------------------------------------------\n",
      "openbmb/MiniCPM-o-4_5                                   | Base         | 9.4B\n",
      "openbmb/MiniCPM-SALA                                    | Base         | 9.5B\n",
      "Nanbeige/Nanbeige4.1-3B                                 | Fine-tuned   | 3.9B\n",
      "inference-net/Schematron-3B                             | Fine-tuned   | 3.2B\n",
      "UCSB-SURFI/VulnLLM-R-7B                                 | Fine-tuned   | 7.6B\n",
      "ytu-ce-cosmos/Turkish-Gemma-9b-T1                       | Fine-tuned   | 9.2B\n",
      "Qwen/Qwen3-ASR-1.7B                                     | Base         | 2.3B\n",
      "kugelaudio/kugelaudio-0-open                            | Base         | 9.3B\n",
      "FutureMa/Eva-4B-V2                                      | Fine-tuned   | 4.0B\n",
      "Octen/Octen-Embedding-8B                                | Fine-tuned   | 7.6B\n",
      "meta-llama/Llama-3.1-8B-Instruct                        | Fine-tuned   | 8.0B\n",
      "Qwen/Qwen3-TTS-12Hz-1.7B-CustomVoice                    | Base         | 1.9B\n",
      "deepseek-ai/DeepSeek-OCR-2                              | Base         | 3.4B\n",
      "PaddlePaddle/PaddleOCR-VL-1.5                           | Fine-tuned   | 958.6M\n",
      "microsoft/VibeVoice-ASR                                 | Base         | 8.7B\n",
      "trillionlabs/gWorld-8B                                  | Base         | 8.8B\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "# Fetch trending models with expanded metadata\n",
    "trending = api.list_models(\n",
    "    sort=\"likes7d\", \n",
    "    limit=50,\n",
    "    expand=[\"safetensors\", \"cardData\"]\n",
    ")\n",
    "\n",
    "# Containers for our tables\n",
    "all_models = []\n",
    "filtered_models = []\n",
    "\n",
    "for model in trending:\n",
    "    m_id = model.id\n",
    "    tags = [t.lower() for t in (model.tags or [])]\n",
    "    \n",
    "    # --- Classification Logic ---\n",
    "    is_lora = \"lora\" in tags or \"peft\" in tags or \"adapter\" in m_id.lower() or \"lora\" in m_id.lower()\n",
    "    is_gguf = \"gguf\" in tags or m_id.lower().endswith(\".gguf\") or \"gguf\" in m_id.lower()\n",
    "    \n",
    "    quant_tags = {\"awq\", \"gptq\", \"exl2\", \"bitsandbytes\", \"quantized\"}\n",
    "    is_quant = any(q in tags for q in quant_tags) or \"quant\" in m_id.lower()\n",
    "    \n",
    "    base_model_pointer = getattr(model.cardData, 'base_model', None)\n",
    "    \n",
    "    if is_lora:\n",
    "        category = \"LoRA\"\n",
    "    elif is_gguf:\n",
    "        category = \"GGUF\"\n",
    "    elif is_quant:\n",
    "        category = \"Quant\"\n",
    "    elif base_model_pointer:\n",
    "        category = \"Fine-tuned\"\n",
    "    else:\n",
    "        category = \"Base\"\n",
    "\n",
    "    # --- Parameter Extraction ---\n",
    "    raw_params = 0\n",
    "    params_display = \"N/A\"\n",
    "    if hasattr(model, 'safetensors') and model.safetensors:\n",
    "        raw_params = model.safetensors.get('total', 0)\n",
    "        if raw_params:\n",
    "            params_display = f\"{raw_params / 1e9:.1f}B\" if raw_params >= 1e9 else f\"{raw_params / 1e6:.1f}M\"\n",
    "\n",
    "    # Store for Table 1\n",
    "    all_models.append((m_id, category, params_display))\n",
    "    \n",
    "    # Store for Table 2 (Base/Fine-tuned only AND < 10B)\n",
    "    # We only include if raw_params > 0 to avoid empty metadata entries\n",
    "    if category in [\"Base\", \"Fine-tuned\"] and 0 < raw_params < 10e9:\n",
    "        filtered_models.append((m_id, category, params_display))\n",
    "\n",
    "# --- Printing Table 1 ---\n",
    "print(\"TABLE 1: TOP 50 TRENDING (ALL TYPES)\")\n",
    "print(f\"{'Model ID':<55} | {'Type':<12} | {'Params'}\")\n",
    "print(\"-\" * 80)\n",
    "for m in all_models:\n",
    "    print(f\"{m[0]:<55} | {m[1]:<12} | {m[2]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# --- Printing Table 2 ---\n",
    "print(\"TABLE 2: BASE & FINE-TUNED MODELS (< 10B PARAMS)\")\n",
    "print(f\"{'Model ID':<55} | {'Type':<12} | {'Params'}\")\n",
    "print(\"-\" * 80)\n",
    "if not filtered_models:\n",
    "    print(\"No models found matching these specific criteria in the current trend list.\")\n",
    "for m in filtered_models:\n",
    "    print(f\"{m[0]:<55} | {m[1]:<12} | {m[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4534dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LOADING TABLE 2 MODELS ON META DEVICE\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encountered exception while importing minicpmo: No module named 'minicpmo'\n",
      "Encountered exception while importing minicpmo: No module named 'minicpmo'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SKIP] openbmb/MiniCPM-o-4_5                                   | Base       | 9.4B | missing dep: minicpmo (pip install minicpmo)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encountered exception while importing fla: No module named 'fla'\n",
      "Encountered exception while importing fla: No module named 'fla'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SKIP] openbmb/MiniCPM-SALA                                    | Base       | 9.5B | missing dep: fla (pip install fla)\n",
      "[OK]   Nanbeige/Nanbeige4.1-3B                                 | Fine-tuned | 3.9B | model_type=llama\n",
      "[OK]   inference-net/Schematron-3B                             | Fine-tuned | 3.2B | model_type=llama\n",
      "[OK]   UCSB-SURFI/VulnLLM-R-7B                                 | Fine-tuned | 7.6B | model_type=qwen2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['cache_implementation']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK]   ytu-ce-cosmos/Turkish-Gemma-9b-T1                       | Fine-tuned | 9.2B | model_type=gemma2\n",
      "[SKIP] Qwen/Qwen3-ASR-1.7B                                     | Base       | 2.3B | unsupported by transformers\n",
      "[SKIP] kugelaudio/kugelaudio-0-open                            | Base       | 9.3B | unsupported by transformers\n",
      "[OK]   FutureMa/Eva-4B-V2                                      | Fine-tuned | 4.0B | model_type=qwen3\n",
      "[OK]   Octen/Octen-Embedding-8B                                | Fine-tuned | 7.6B | model_type=qwen3\n",
      "[FAIL] meta-llama/Llama-3.1-8B-Instruct                        | Fine-tuned | 8.0B | OSError: You are trying to access a gated repo.\n",
      "Make sure to have access to it at https://huggingface.co/meta-llama/Llama-3.1-8B-\n",
      "[SKIP] Qwen/Qwen3-TTS-12Hz-1.7B-CustomVoice                    | Base       | 1.9B | unsupported by transformers\n",
      "[FAIL] deepseek-ai/DeepSeek-OCR-2                              | Base       | 3.4B | ImportError: cannot import name 'LlamaFlashAttention2' from 'transformers.models.llama.modeling_llama' (c:\\Users\\emman\\Desktop\\PROYEC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\emman\\Desktop\\PROYECTOS_VS_CODE\\GITHUB\\Model-Quantization\\venv\\Lib\\site-packages\\transformers\\modeling_rope_utils.py:936: FutureWarning: `rope_config_validation` is deprecated and has been removed. Its functionality has been moved to RotaryEmbeddingConfigMixin.validate_rope method. PreTrainedConfig inherits this class, so please call self.validate_rope() instead. Also, make sure to use the new rope_parameters syntax. You can call self.standardize_rope_params() in the meantime.\n",
      "  warnings.warn(\n",
      "Unrecognized keys in `rope_parameters` for 'rope_type'='default': {'mrope_section'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SKIP] PaddlePaddle/PaddleOCR-VL-1.5                           | Fine-tuned | 958.6M | model_type=paddleocr_vl\n",
      "[SKIP] microsoft/VibeVoice-ASR                                 | Base       | 8.7B | unsupported by transformers\n",
      "[SKIP] trillionlabs/gWorld-8B                                  | Base       | 8.8B | model_type=qwen3_vl\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoModel\n",
    "\n",
    "SKIP_MODEL_TYPES = {\n",
    "    \"qwen3_vl\",\n",
    "    \"paddleocr_vl\",\n",
    "    \"vibevoice\",      # unsupported in transformers right now\n",
    "}\n",
    "\n",
    "SKIP_KEYWORDS = (\"vl\", \"vision\", \"multimodal\", \"asr\", \"ocr\", \"speech\")\n",
    "\n",
    "def extract_missing_package(err: Exception):\n",
    "    msg = str(err)\n",
    "    m = re.search(r\"packages that were not found.*?:\\s*([a-zA-Z0-9_\\-]+)\", msg)\n",
    "    if m:\n",
    "        return m.group(1)\n",
    "    m = re.search(r\"No module named '([^']+)'\", msg)\n",
    "    if m:\n",
    "        return m.group(1)\n",
    "    return None\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOADING TABLE 2 MODELS ON META DEVICE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "loaded_meta_models = {}\n",
    "\n",
    "for model_id, category, params in filtered_models:\n",
    "    try:\n",
    "        config = AutoConfig.from_pretrained(model_id, trust_remote_code=True)\n",
    "        model_type = (getattr(config, \"model_type\", \"\") or \"\").lower()\n",
    "\n",
    "        # Skip known non-text or unsupported types\n",
    "        if model_type in SKIP_MODEL_TYPES:\n",
    "            print(f\"[SKIP] {model_id:<55} | {category:<10} | {params} | model_type={model_type}\")\n",
    "            continue\n",
    "\n",
    "        # Optional heuristic skip (helps avoid OCR/ASR/VL even if model_type is generic)\n",
    "        if any(k in model_id.lower() for k in SKIP_KEYWORDS):\n",
    "            print(f\"[SKIP] {model_id:<55} | {category:<10} | {params} | heuristic_skip\")\n",
    "            continue\n",
    "\n",
    "        with torch.device(\"meta\"):\n",
    "            try:\n",
    "                model = AutoModelForCausalLM.from_config(config, trust_remote_code=True)\n",
    "            except Exception:\n",
    "                model = AutoModel.from_config(config, trust_remote_code=True)\n",
    "\n",
    "        loaded_meta_models[model_id] = model\n",
    "        print(f\"[OK]   {model_id:<55} | {category:<10} | {params} | model_type={model_type}\")\n",
    "\n",
    "    except ImportError as e:\n",
    "        missing = extract_missing_package(e)\n",
    "        if missing:\n",
    "            print(f\"[SKIP] {model_id:<55} | {category:<10} | {params} | missing dep: {missing} (pip install {missing})\")\n",
    "        else:\n",
    "            print(f\"[FAIL] {model_id:<55} | {category:<10} | {params} | ImportError: {str(e)[:120]}\")\n",
    "\n",
    "    except ValueError as e:\n",
    "        # Common case: unknown architecture\n",
    "        if \"does not recognize this architecture\" in str(e).lower():\n",
    "            print(f\"[SKIP] {model_id:<55} | {category:<10} | {params} | unsupported by transformers\")\n",
    "        else:\n",
    "            print(f\"[FAIL] {model_id:<55} | {category:<10} | {params} | ValueError: {str(e)[:120]}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[FAIL] {model_id:<55} | {category:<10} | {params} | {type(e).__name__}: {str(e)[:120]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00fdffe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
